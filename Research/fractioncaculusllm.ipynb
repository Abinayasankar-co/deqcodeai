{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractional Power of A^0.5:\n",
      "[[1.98157763 0.27083221]\n",
      " [0.27083221 1.71074543]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "def fractional_power_matrix(A, alpha):\n",
    "    \"\"\"\n",
    "    Compute the fractional power of a matrix A to the power alpha.\n",
    "    Args:\n",
    "        A (np.array): The input square matrix.\n",
    "        alpha (float): The fractional power to which the matrix should be raised.\n",
    "    Returns:\n",
    "        np.array: The fractional power of the matrix.\n",
    "    \"\"\"\n",
    "    if not np.allclose(A, A.T):\n",
    "        raise ValueError(\"Matrix must be symmetric for fractional powers.\")\n",
    "    return fractional_matrix_power(A, alpha)\n",
    "\n",
    "# Example usage\n",
    "A = np.array([[4, 1], [1, 3]])\n",
    "alpha = 0.5  # Example fractional power\n",
    "A_fractional = fractional_power_matrix(A, alpha)\n",
    "print(\"Fractional Power of A^0.5:\")\n",
    "print(A_fractional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from Fractional Attention Layer: torch.Size([2, 4, 8])\n",
      "tensor([[[ 0.1760,  0.0748, -0.8958,  0.3870, -0.1142,  0.3949, -0.1325,\n",
      "           0.1630],\n",
      "         [ 0.1727,  0.0773, -0.8998,  0.3854, -0.1154,  0.3950, -0.1323,\n",
      "           0.1638],\n",
      "         [ 0.1763,  0.0746, -0.8938,  0.3874, -0.1151,  0.3941, -0.1317,\n",
      "           0.1630],\n",
      "         [ 0.1787,  0.0728, -0.8896,  0.3887, -0.1152,  0.3931, -0.1312,\n",
      "           0.1624]],\n",
      "\n",
      "        [[ 0.3159, -0.3146, -0.6623,  0.5485,  0.0555,  0.3512, -0.0403,\n",
      "           0.3671],\n",
      "         [ 0.3172, -0.3143, -0.6630,  0.5481,  0.0567,  0.3498, -0.0368,\n",
      "           0.3687],\n",
      "         [ 0.3152, -0.3135, -0.6644,  0.5494,  0.0543,  0.3512, -0.0444,\n",
      "           0.3657],\n",
      "         [ 0.3169, -0.3134, -0.6633,  0.5480,  0.0560,  0.3498, -0.0385,\n",
      "           0.3677]]], grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FractionalAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Initialize a fractional attention module.\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of embeddings.\n",
    "            alpha (float): Fractional power for the attention weights.\n",
    "        \"\"\"\n",
    "        super(FractionalAttention, self).__init__()\n",
    "        self.query = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for fractional attention.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Attention computation (dot product)\n",
    "        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / (Q.size(-1) ** 0.5)\n",
    "        \n",
    "        # Softmax to get probabilities\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Fractional power of the attention probabilities\n",
    "        fractional_attention_probs = torch.pow(attention_probs, self.alpha)\n",
    "        \n",
    "        # Multiply the attention weights by the value\n",
    "        output = torch.bmm(fractional_attention_probs, V)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "embed_dim = 8\n",
    "x = torch.rand(batch_size, seq_len, embed_dim)\n",
    "attention = FractionalAttention(embed_dim=embed_dim, alpha=0.8)\n",
    "output = attention(x)\n",
    "print(\"Output from Fractional Attention Layer:\", output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5563, -2.5354, -6.7174],\n",
       "        [ 0.7297, -0.0386,  2.5425],\n",
       "        [-4.2383, -2.0661, -4.6462],\n",
       "        [-1.9240, -2.8116, -2.7333],\n",
       "        [-3.0439, -1.3420, -3.8347],\n",
       "        [ 0.7035,  0.5866, -2.1987],\n",
       "        [ 0.3729, -1.7287,  0.3559],\n",
       "        [ 0.5532,  2.7181,  1.1819],\n",
       "        [ 2.4136,  3.3529, -0.3646],\n",
       "        [-1.4591, -0.9820, -3.9150]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class FractionalLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, alpha=0.9):\n",
    "        \"\"\"\n",
    "        Initialize a linear layer with fractional backpropagation.\n",
    "        Args:\n",
    "            in_features (int): Number of input features.\n",
    "            out_features (int): Number of output features.\n",
    "            alpha (float): Fractional order of the gradient.\n",
    "        \"\"\"\n",
    "        super(FractionalLinear, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight.T) + self.bias\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Custom backward function using fractional derivatives.\n",
    "        Args:\n",
    "            grad_output (torch.Tensor): The gradient of the loss.\n",
    "        Returns:\n",
    "            torch.Tensor: Fractional gradient.\n",
    "        \"\"\"\n",
    "        grad_input = torch.pow(grad_output, self.alpha)\n",
    "        return grad_input\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(10, 5)\n",
    "layer = FractionalLinear(5, 3, alpha=0.8)\n",
    "output = layer(x)\n",
    "loss = torch.sum(output)\n",
    "loss.backward()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits: torch.Size([2, 10, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.4700e-01, -3.1152e-01,  4.8622e-02,  ..., -1.6796e-03,\n",
       "          -1.4546e-01, -5.1911e-01],\n",
       "         [-3.1036e-01,  8.2553e-01, -1.5342e-01,  ..., -5.8901e-01,\n",
       "           3.7547e-01,  2.9392e-01],\n",
       "         [ 5.8428e-01,  1.0994e-01, -1.1075e-01,  ...,  3.3377e-01,\n",
       "           1.0201e-01, -1.9540e-01],\n",
       "         ...,\n",
       "         [ 1.9379e-02,  4.7495e-01,  3.9328e-01,  ..., -1.4321e-01,\n",
       "           6.3253e-01, -9.5563e-01],\n",
       "         [ 5.9369e-01, -1.0885e-01,  1.9067e-01,  ..., -4.3912e-02,\n",
       "           1.3999e-01, -9.0806e-01],\n",
       "         [ 7.2416e-03,  2.9712e-01, -3.1221e-01,  ..., -1.4748e-01,\n",
       "          -3.2595e-01,  1.2130e-01]],\n",
       "\n",
       "        [[ 5.3162e-01,  6.1521e-03, -3.3096e-02,  ...,  4.4141e-01,\n",
       "          -2.7279e-01, -2.0244e-02],\n",
       "         [ 2.7498e-01, -3.4813e-01,  4.4897e-01,  ...,  3.4805e-01,\n",
       "           2.4139e-01, -5.0521e-01],\n",
       "         [ 2.0578e-01,  1.2037e-01, -1.0812e-01,  ..., -1.2177e-01,\n",
       "           2.4607e-01,  4.9796e-01],\n",
       "         ...,\n",
       "         [ 2.3163e-01,  4.9736e-01, -2.5494e-01,  ..., -9.1786e-02,\n",
       "           9.6683e-03,  3.2564e-01],\n",
       "         [ 1.8356e-01, -6.1955e-01,  1.0318e-01,  ...,  9.4688e-04,\n",
       "           4.9967e-01, -1.4241e-01],\n",
       "         [ 3.0178e-01,  5.6842e-01, -8.4065e-02,  ..., -1.0133e+00,\n",
       "           6.6701e-01,  5.1001e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FractionalTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, alpha=0.8):\n",
    "        super(FractionalTransformerBlock, self).__init__()\n",
    "        self.attention = FractionalAttention(embed_dim, alpha)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            FractionalLinear(embed_dim, embed_dim * 4, alpha),\n",
    "            nn.ReLU(),\n",
    "            FractionalLinear(embed_dim * 4, embed_dim, alpha)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "class FractionalGPT(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, vocab_size, alpha=0.8):\n",
    "        super(FractionalGPT, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            FractionalTransformerBlock(embed_dim, num_heads, alpha) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        logits = self.output_head(x)\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 1000\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "embed_dim = 32\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "model = FractionalGPT(num_layers, embed_dim, num_heads, vocab_size, alpha=0.8)\n",
    "logits = model(x)\n",
    "print(\"Output logits:\", logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\python312\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\python312\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\python312\\lib\\site-packages (from datasets) (0.23.5)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abinaya sankar m\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "   ---------------------------------------- 0.0/480.6 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 61.4/480.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 307.2/480.6 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 480.6/480.6 kB 4.3 MB/s eta 0:00:00\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.7/25.1 MB 20.8 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.6/25.1 MB 20.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.8/25.1 MB 22.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 3.9/25.1 MB 22.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.9/25.1 MB 22.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 5.9/25.1 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.8/25.1 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.9/25.1 MB 21.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 8.8/25.1 MB 21.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.8/25.1 MB 21.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 10.8/25.1 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.7/25.1 MB 21.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.7/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.7/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.6/25.1 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.6/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.6/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.6/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.6/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.6/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.5/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.4/25.1 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/25.1 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.1 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
      "Successfully installed datasets-3.2.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-18.1.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angsmith (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angsmith (C:\\Python312\\Lib\\site-packages)\n",
      "  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\ABINAYA SANKAR M\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"glaiveai/glaive-code-assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ======= Data Preparation =======\n",
    "def tokenize(text, vocab):\n",
    "    \"\"\"Simple tokenization function to convert text to token indices\"\"\"\n",
    "    return [vocab.get(char, vocab['<unk>']) for char in text]\n",
    "\n",
    "def prepare_data(texts, vocab, seq_len):\n",
    "    \"\"\"Converts multiple texts into training data (X, Y)\"\"\"\n",
    "    tokenized_data = [tokenize(text, vocab) for text in texts]\n",
    "    X, Y = [], []\n",
    "    for tokens in tokenized_data:\n",
    "        for i in range(len(tokens) - seq_len):\n",
    "            X.append(tokens[i:i + seq_len])  # Input\n",
    "            Y.append(tokens[i + 1:i + seq_len + 1])  # Target\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "# Model Definition\n",
    "class FractionalAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        attn_output = attn_output ** self.alpha  # Fractional power of attention weights\n",
    "        return attn_output\n",
    "\n",
    "class FractionalTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.attention = FractionalAttention(embed_dim, num_heads, alpha)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attention(x)\n",
    "        x = x + attn_out  # Residual connection\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + ffn_out  # Residual connection\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class FractionalGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, seq_len, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            FractionalTransformerBlock(embed_dim, num_heads, alpha) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        logits = self.output_head(x)\n",
    "        return logits\n",
    "\n",
    "# Model Training \n",
    "def train_model(model, data_loader, optimizer, loss_fn, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for X, Y in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
    "            loss.backward()  # Fractional gradients are applied automatically\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
