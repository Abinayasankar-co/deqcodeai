{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractional Power of A^0.5:\n",
      "[[1.98157763 0.27083221]\n",
      " [0.27083221 1.71074543]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "def fractional_power_matrix(A, alpha):\n",
    "    \"\"\"\n",
    "    Compute the fractional power of a matrix A to the power alpha.\n",
    "    Args:\n",
    "        A (np.array): The input square matrix.\n",
    "        alpha (float): The fractional power to which the matrix should be raised.\n",
    "    Returns:\n",
    "        np.array: The fractional power of the matrix.\n",
    "    \"\"\"\n",
    "    if not np.allclose(A, A.T):\n",
    "        raise ValueError(\"Matrix must be symmetric for fractional powers.\")\n",
    "    return fractional_matrix_power(A, alpha)\n",
    "\n",
    "# Example usage\n",
    "A = np.array([[4, 1], [1, 3]])\n",
    "alpha = 0.5  # Example fractional power\n",
    "A_fractional = fractional_power_matrix(A, alpha)\n",
    "print(\"Fractional Power of A^0.5:\")\n",
    "print(A_fractional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from Fractional Attention Layer: torch.Size([2, 4, 8])\n",
      "tensor([[[ 0.1760,  0.0748, -0.8958,  0.3870, -0.1142,  0.3949, -0.1325,\n",
      "           0.1630],\n",
      "         [ 0.1727,  0.0773, -0.8998,  0.3854, -0.1154,  0.3950, -0.1323,\n",
      "           0.1638],\n",
      "         [ 0.1763,  0.0746, -0.8938,  0.3874, -0.1151,  0.3941, -0.1317,\n",
      "           0.1630],\n",
      "         [ 0.1787,  0.0728, -0.8896,  0.3887, -0.1152,  0.3931, -0.1312,\n",
      "           0.1624]],\n",
      "\n",
      "        [[ 0.3159, -0.3146, -0.6623,  0.5485,  0.0555,  0.3512, -0.0403,\n",
      "           0.3671],\n",
      "         [ 0.3172, -0.3143, -0.6630,  0.5481,  0.0567,  0.3498, -0.0368,\n",
      "           0.3687],\n",
      "         [ 0.3152, -0.3135, -0.6644,  0.5494,  0.0543,  0.3512, -0.0444,\n",
      "           0.3657],\n",
      "         [ 0.3169, -0.3134, -0.6633,  0.5480,  0.0560,  0.3498, -0.0385,\n",
      "           0.3677]]], grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FractionalAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Initialize a fractional attention module.\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of embeddings.\n",
    "            alpha (float): Fractional power for the attention weights.\n",
    "        \"\"\"\n",
    "        super(FractionalAttention, self).__init__()\n",
    "        self.query = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for fractional attention.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Attention computation (dot product)\n",
    "        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / (Q.size(-1) ** 0.5)\n",
    "        \n",
    "        # Softmax to get probabilities\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Fractional power of the attention probabilities\n",
    "        fractional_attention_probs = torch.pow(attention_probs, self.alpha)\n",
    "        \n",
    "        # Multiply the attention weights by the value\n",
    "        output = torch.bmm(fractional_attention_probs, V)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "embed_dim = 8\n",
    "x = torch.rand(batch_size, seq_len, embed_dim)\n",
    "attention = FractionalAttention(embed_dim=embed_dim, alpha=0.8)\n",
    "output = attention(x)\n",
    "print(\"Output from Fractional Attention Layer:\", output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5563, -2.5354, -6.7174],\n",
       "        [ 0.7297, -0.0386,  2.5425],\n",
       "        [-4.2383, -2.0661, -4.6462],\n",
       "        [-1.9240, -2.8116, -2.7333],\n",
       "        [-3.0439, -1.3420, -3.8347],\n",
       "        [ 0.7035,  0.5866, -2.1987],\n",
       "        [ 0.3729, -1.7287,  0.3559],\n",
       "        [ 0.5532,  2.7181,  1.1819],\n",
       "        [ 2.4136,  3.3529, -0.3646],\n",
       "        [-1.4591, -0.9820, -3.9150]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class FractionalLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, alpha=0.9):\n",
    "        \"\"\"\n",
    "        Initialize a linear layer with fractional backpropagation.\n",
    "        Args:\n",
    "            in_features (int): Number of input features.\n",
    "            out_features (int): Number of output features.\n",
    "            alpha (float): Fractional order of the gradient.\n",
    "        \"\"\"\n",
    "        super(FractionalLinear, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight.T) + self.bias\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Custom backward function using fractional derivatives.\n",
    "        Args:\n",
    "            grad_output (torch.Tensor): The gradient of the loss.\n",
    "        Returns:\n",
    "            torch.Tensor: Fractional gradient.\n",
    "        \"\"\"\n",
    "        grad_input = torch.pow(grad_output, self.alpha)\n",
    "        return grad_input\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(10, 5)\n",
    "layer = FractionalLinear(5, 3, alpha=0.8)\n",
    "output = layer(x)\n",
    "loss = torch.sum(output)\n",
    "loss.backward()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits: torch.Size([2, 10, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.4700e-01, -3.1152e-01,  4.8622e-02,  ..., -1.6796e-03,\n",
       "          -1.4546e-01, -5.1911e-01],\n",
       "         [-3.1036e-01,  8.2553e-01, -1.5342e-01,  ..., -5.8901e-01,\n",
       "           3.7547e-01,  2.9392e-01],\n",
       "         [ 5.8428e-01,  1.0994e-01, -1.1075e-01,  ...,  3.3377e-01,\n",
       "           1.0201e-01, -1.9540e-01],\n",
       "         ...,\n",
       "         [ 1.9379e-02,  4.7495e-01,  3.9328e-01,  ..., -1.4321e-01,\n",
       "           6.3253e-01, -9.5563e-01],\n",
       "         [ 5.9369e-01, -1.0885e-01,  1.9067e-01,  ..., -4.3912e-02,\n",
       "           1.3999e-01, -9.0806e-01],\n",
       "         [ 7.2416e-03,  2.9712e-01, -3.1221e-01,  ..., -1.4748e-01,\n",
       "          -3.2595e-01,  1.2130e-01]],\n",
       "\n",
       "        [[ 5.3162e-01,  6.1521e-03, -3.3096e-02,  ...,  4.4141e-01,\n",
       "          -2.7279e-01, -2.0244e-02],\n",
       "         [ 2.7498e-01, -3.4813e-01,  4.4897e-01,  ...,  3.4805e-01,\n",
       "           2.4139e-01, -5.0521e-01],\n",
       "         [ 2.0578e-01,  1.2037e-01, -1.0812e-01,  ..., -1.2177e-01,\n",
       "           2.4607e-01,  4.9796e-01],\n",
       "         ...,\n",
       "         [ 2.3163e-01,  4.9736e-01, -2.5494e-01,  ..., -9.1786e-02,\n",
       "           9.6683e-03,  3.2564e-01],\n",
       "         [ 1.8356e-01, -6.1955e-01,  1.0318e-01,  ...,  9.4688e-04,\n",
       "           4.9967e-01, -1.4241e-01],\n",
       "         [ 3.0178e-01,  5.6842e-01, -8.4065e-02,  ..., -1.0133e+00,\n",
       "           6.6701e-01,  5.1001e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FractionalTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, alpha=0.8):\n",
    "        super(FractionalTransformerBlock, self).__init__()\n",
    "        self.attention = FractionalAttention(embed_dim, alpha)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            FractionalLinear(embed_dim, embed_dim * 4, alpha),\n",
    "            nn.ReLU(),\n",
    "            FractionalLinear(embed_dim * 4, embed_dim, alpha)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "class FractionalGPT(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, vocab_size, alpha=0.8):\n",
    "        super(FractionalGPT, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            FractionalTransformerBlock(embed_dim, num_heads, alpha) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        logits = self.output_head(x)\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 1000\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "embed_dim = 32\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "model = FractionalGPT(num_layers, embed_dim, num_heads, vocab_size, alpha=0.8)\n",
    "logits = model(x)\n",
    "print(\"Output logits:\", logits.shape)\n",
    "logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
