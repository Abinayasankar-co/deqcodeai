{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractionalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.d_model = d_model\n",
    "        self.positional_embeddings = self.generate_positional_embeddings(d_model, max_len)\n",
    "    \n",
    "    def generate_positional_embeddings(self, d_model, max_len):\n",
    "        positions = torch.arange(0, max_len).unsqueeze(1)\n",
    "        freq = torch.pow(10000, -2 * (torch.arange(0, d_model, 2) / d_model))\n",
    "        embeddings = positions * freq\n",
    "        embeddings = torch.sin(embeddings) + torch.cos(embeddings) ** self.alpha\n",
    "        embeddings = embeddings.unsqueeze(1)  # Shape (max_len, 1, d_model)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # Expand x to have 3 dimensions: (batch_size, seq_len, d_model)\n",
    "        x = x.unsqueeze(-1).repeat(1, 1, self.d_model)  # Shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Get the positional embedding for this specific sequence length\n",
    "        positional_emb = self.positional_embeddings[:seq_len, 0, :].unsqueeze(0).repeat(batch_size, 1, 1).to(x.device)\n",
    "        print(f\"positional_emb: {positional_emb.shape}\")\n",
    "        positional_emb = positional_emb.repeat_interleave(2,dim=-1)\n",
    "        print(f\"positional_emb: {positional_emb.shape}\")\n",
    "        \n",
    "        # Check tensor shapes before addition\n",
    "        print(f\"x.shape: {x.shape}, positional_emb.shape: {positional_emb.shape}\")\n",
    "        \n",
    "        # Ensure the tensors have the same shape before addition\n",
    "        return x + positional_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Try 3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FractionalAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads, alpha=0.8, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon  # Small epsilon for numerical stability\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        Q = self.query(q)\n",
    "        K = self.key(k)\n",
    "        V = self.value(v)\n",
    "\n",
    "        # Compute attention scores with numerical stability\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))\n",
    "        \n",
    "        try:\n",
    "            # Add small diagonal to improve condition number\n",
    "            attention_scores = attention_scores + self.epsilon * torch.eye(attention_scores.size(-1), \n",
    "                                                                           device=attention_scores.device)\n",
    "            \n",
    "            # Safe eigendecomposition with robust method\n",
    "            eigvals, eigvecs = torch.linalg.eigh(attention_scores)\n",
    "            \n",
    "            # Clip eigenvalues to prevent overflow\n",
    "            eigvals = torch.clamp(eigvals, min=self.epsilon)\n",
    "            \n",
    "            # Fractional power with clipped values\n",
    "            fractional_eigvals = torch.pow(eigvals, self.alpha)\n",
    "            \n",
    "            # Reconstruct matrix\n",
    "            attention_scores = eigvecs @ torch.diag_embed(fractional_eigvals) @ eigvecs.transpose(-2, -1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Fallback to a stable method if eigendecomposition fails\n",
    "            print(f\"Eigendecomposition failed: {e}\")\n",
    "            attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Normalize attention weights\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        return torch.matmul(attention_weights, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractionalCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        fractional_log_probs = log_probs ** self.alpha\n",
    "        return -torch.mean(torch.sum(fractional_log_probs * targets, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractionalGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.embedding = FractionalPositionalEmbedding(d_model, alpha=alpha)\n",
    "        self.layers = nn.ModuleList([FractionalAttention(d_model, n_heads, alpha=alpha) for _ in range(n_layers)])\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractionalGradientDescent(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.8):\n",
    "        defaults = dict(lr=lr, alpha=alpha)\n",
    "        super(FractionalGradientDescent, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    alpha = group['alpha']\n",
    "                    grad = p.grad.data\n",
    "                    fractional_grad = grad ** alpha\n",
    "                    p.data -= group['lr'] * fractional_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_emb: torch.Size([32, 20, 32])\n",
      "positional_emb: torch.Size([32, 20, 64])\n",
      "x.shape: torch.Size([32, 20, 64]), positional_emb.shape: torch.Size([32, 20, 64])\n",
      "Eigendecomposition failed: linalg.eigh: (Batch element 0): The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 19).\n",
      "Eigendecomposition failed: linalg.eigh: (Batch element 0): The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 19).\n",
      "Eigendecomposition failed: linalg.eigh: (Batch element 0): The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 19).\n",
      "Eigendecomposition failed: linalg.eigh: (Batch element 0): The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 19).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000) must match the size of tensor b (640) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrain_fractional_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 15\u001b[0m, in \u001b[0;36mtrain_fractional_gpt\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m, in \u001b[0;36mFractionalCrossEntropyLoss.forward\u001b[1;34m(self, logits, targets)\u001b[0m\n\u001b[0;32m      7\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m fractional_log_probs \u001b[38;5;241m=\u001b[39m log_probs \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mfractional_log_probs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (640) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def train_fractional_gpt():\n",
    "    model = FractionalGPT(vocab_size=1000, d_model=64, n_layers=4, n_heads=4, alpha=0.8)\n",
    "    criterion = FractionalCrossEntropyLoss(alpha=0.8)\n",
    "    optimizer = FractionalGradientDescent(model.parameters(), lr=1e-4, alpha=0.8)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # Inputs: batch_size x seq_len\n",
    "        inputs = torch.randint(0, 1000, (32, 20))\n",
    "        targets = torch.randint(0, 1000, (32, 20))\n",
    "\n",
    "        # Ensure model input matches expected shape\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits.view(-1, 1000), targets.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "train_fractional_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fractional\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FractionalAttention(nn.Module):\n",
    "    def __init__(self, d_model, alpha=0.8, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def robust_eigendecomposition(self, attention_scores):\n",
    "        try:\n",
    "            # Method 1: Stabilized Eigendecomposition\n",
    "            stabilized_scores = attention_scores + self.epsilon * torch.eye(\n",
    "                attention_scores.size(-1), \n",
    "                device=attention_scores.device, \n",
    "                dtype=attention_scores.dtype\n",
    "            )\n",
    "            \n",
    "            eigvals, eigvecs = torch.linalg.eigh(stabilized_scores)\n",
    "            \n",
    "            # Clip and process eigenvalues\n",
    "            eigvals = torch.clamp(eigvals, min=self.epsilon)\n",
    "            fractional_eigvals = torch.pow(eigvals, self.alpha)\n",
    "            \n",
    "            reconstructed_scores = eigvecs @ torch.diag_embed(fractional_eigvals) @ eigvecs.transpose(-2, -1)\n",
    "            \n",
    "            return reconstructed_scores\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback to SVD if eigendecomposition fails\n",
    "            try:\n",
    "                U, S, Vh = torch.linalg.svd(attention_scores)\n",
    "                S_clamped = torch.clamp(S, min=self.epsilon)\n",
    "                S_fractional = torch.pow(S_clamped, self.alpha)\n",
    "                \n",
    "                reconstructed_scores = U @ torch.diag_embed(S_fractional) @ Vh\n",
    "                \n",
    "                return reconstructed_scores\n",
    "\n",
    "            except Exception:\n",
    "                # Final fallback to softmax\n",
    "                return torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # Linear projections\n",
    "        Q = self.query(q)\n",
    "        K = self.key(k)\n",
    "        V = self.value(v)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))\n",
    "        \n",
    "        # Robust Eigendecomposition\n",
    "        processed_scores = self.robust_eigendecomposition(attention_scores)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.nn.functional.softmax(processed_scores, dim=-1)\n",
    "        \n",
    "        # Final output\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1 (Repeat Interleave) shape: torch.Size([32, 30, 64])\n",
      "Method 2 (Zero Padding) shape: torch.Size([32, 30, 64])\n",
      "Method 3 (Concatenation) shape: torch.Size([32, 30, 64])\n",
      "Method 4 (Interpolation) shape: torch.Size([32, 30, 64])\n",
      "Method 5 (Tiling) shape: torch.Size([32, 30, 64])\n"
     ]
    }
   ],
   "source": [
    "#Error Handling Mechnaisms\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Original tensor with shape [32, 30, 32]\n",
    "positional_emb = torch.randn(32, 30, 32)\n",
    "\n",
    "# Method 1: Simple Repetition\n",
    "repeated_emb = positional_emb.repeat_interleave(2, dim=-1)\n",
    "\n",
    "# Method 2: Zero Padding\n",
    "zero_padded = F.pad(positional_emb, (0, 32))\n",
    "\n",
    "# Method 3: Concatenation with itself\n",
    "concat_self = torch.cat([positional_emb, positional_emb], dim=-1)\n",
    "\n",
    "# Method 4: Using interpolation techniques\n",
    "def custom_interpolation(tensor, target_dim):\n",
    "    # Linear interpolation between original values\n",
    "    original = tensor\n",
    "    expanded = torch.zeros(tensor.shape[0], tensor.shape[1], target_dim, device=tensor.device)\n",
    "    expanded[:, :, :tensor.shape[-1]] = original\n",
    "    \n",
    "    # Fill the rest with interpolated values\n",
    "    for i in range(tensor.shape[-1], target_dim):\n",
    "        # Simple linear interpolation\n",
    "        prev = expanded[:, :, i-1]\n",
    "        expanded[:, :, i] = prev * 1.1  # Simple scaling\n",
    "    \n",
    "    return expanded\n",
    "\n",
    "interpolated_emb = custom_interpolation(positional_emb, 64)\n",
    "\n",
    "# Method 5: Tiling with a pattern\n",
    "tiled_emb = torch.tile(positional_emb, (1, 1, 2))\n",
    "\n",
    "# Verify shape\n",
    "print(\"Method 1 (Repeat Interleave) shape:\", repeated_emb.shape)\n",
    "print(\"Method 2 (Zero Padding) shape:\", zero_padded.shape)\n",
    "print(\"Method 3 (Concatenation) shape:\", concat_self.shape)\n",
    "print(\"Method 4 (Interpolation) shape:\", interpolated_emb.shape)\n",
    "print(\"Method 5 (Tiling) shape:\", tiled_emb.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
